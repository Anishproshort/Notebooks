{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0665a80-6137-4a61-a3da-93bde606df04",
   "metadata": {},
   "source": [
    "Let's load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_file = \"New_data.json\"\n",
    "\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9618cd92-acdd-471b-9521-d55c38af8040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.',\n",
       "   'input': \" In a memorable opening match, Germany triumphed over Scotland, leaving the Scots in despair. The final score: Germany 5, Scotland 1. After Wirtz scored in the 10th minute, the game turned into a nightmare for Scotland.   Germany's victory was a history-making event, with Musiala, Havertz, Füllkrug, and Can scoring four additional goals. Scotland's defeat was sealed when Ryan Porteous was sent off, leaving Havertz to score from the penalty spot.   This match was unique in the history of the European Championships, featuring an own goal, a penalty goal, and a red card. Jamal Musiala, who scored Germany's second goal, was named UEFA's Player of the Match.   For the first time in Euro history, a coach, Julian Nagelsmann, fielded a player older than him, Manuel Neuer. This match marked Scotland's worst defeat since their friendly against the USA in 2012.  \",\n",
       "   'output': '[\\'Germany vs Scotland football match\\', \\'Musiala\\', \\'Havertz\\', \\'Füllkrug\\', \\'and Can scoring\\', \"Jamal Musiala UEFA\\'s Player of the Match\", \\'Julian Nagelsmann and Manuel Neuer\\']'},\n",
       "  {'instruction': 'Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.',\n",
       "   'input': \" Avadh Ojha shares his unfiltered thoughts on various topics including youth, politics, and the education system in a video presented by AJIO.   He discusses the challenges faced by today's youth in a rapidly changing world, emphasizing the importance of adaptability and resilience.   Ojha also delves into the complexities of politics, highlighting the need for transparency and accountability in governance.   He criticizes the current state of the education system, calling for reforms to better prepare students for the future.   Ojha's insightful commentary encourages viewers to critically analyze these issues and work towards creating positive change in society.   His perspective serves as a call to action for individuals to become more informed and actively involved in shaping a better future for themselves and future generations.  \",\n",
       "   'output': \"['Avadh Ojha speaking', 'challenges faced by youth', 'transparency in politics', 'education system reforms', 'positive change in society', 'shaping a better future']\"},\n",
       "  {'instruction': 'Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.',\n",
       "   'input': ' In the world of wealth and influence, these individuals stand out as titans of industry and innovation.   Each person on this list has not only amassed extraordinary wealth but has also shaped global economies, industries, and technological advancements.   Jeff Bezos, the visionary behind Amazon, continues to redefine e-commerce and cloud computing, fundamentally changing how we shop and compute.   Elon Musk, with his ventures Tesla and SpaceX, pushes boundaries in electric vehicles and space exploration, inspiring generations with his bold visions for the future.   Bernard Arnault, leading the luxury sector with brands like Louis Vuitton and Dior, epitomizes elegance and entrepreneurship on a global scale.   Bill Gates, co-founder of Microsoft and a leading philanthropist, remains dedicated to improving global health and education through his foundation.   Mark Zuckerberg, the driving force behind Facebook, connects billions worldwide and shapes social interaction in unprecedented ways.   These individuals not only symbolize wealth but also wield immense influence over technology, retail, luxury, and philanthropy, driving innovation and setting benchmarks for success in the modern era.  ',\n",
       "   'output': '[\"world\\'s top richest people\", \\'global economies and industries\\', \\'Jeff Bezos Amazon\\', \\'Elon Musk Tesla SpaceX\\', \\'Bernard Arnault Louis Vuitton Dior\\', \\'Bill Gates Microsoft\\', \\'Mark Zuckerberg Facebook\\', \\'influence over technology retail luxury philanthropy\\']'}],\n",
       " 9641)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3], len(alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9786466-4012-4cc4-8f9a-e673c74aa965",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manishproshort\u001b[0m (\u001b[33mps-ml-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/FineTuning/wandb/run-20240619_051556-5sx8abco</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/5sx8abco' target=\"_blank\">curious-deluge-25</a></strong> to <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/5sx8abco' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/5sx8abco</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='31.893 MB of 31.893 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">curious-deluge-25</strong> at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/5sx8abco' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/5sx8abco</a><br/> View project at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_051556-5sx8abco/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# log to wandb\n",
    "with wandb.init(project=\"data_ft_phi3LoRA\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"data\", \n",
    "        type=\"dataset\",\n",
    "        description=\"Using the data we finetune\",\n",
    "    )\n",
    "    at.add_file(dataset_file)\n",
    "\n",
    "    # log as a table\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
    "    for row in alpaca:\n",
    "        table.add_data(*row.values())\n",
    "    wandb.log({\"data_table\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfa1958-c58d-4b40-a7d9-5e0cc6d2abf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(alpaca)  # this could also be a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = alpaca[:-1000]\n",
    "eval_dataset = alpaca[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbc0fd0-de6c-447d-abb9-3176c6ceeaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/FineTuning/wandb/run-20240619_051614-6j5sckz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/6j5sckz2' target=\"_blank\">serene-galaxy-26</a></strong> to <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/6j5sckz2' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/6j5sckz2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='48.097 MB of 48.097 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">serene-galaxy-26</strong> at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/6j5sckz2' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/6j5sckz2</a><br/> View project at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a><br/>Synced 5 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_051614-6j5sckz2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "\n",
    "train_table = wandb.Table(dataframe=train_df)\n",
    "eval_table  = wandb.Table(dataframe=eval_df)\n",
    "\n",
    "train_df.to_json(\"data_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"data_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "with wandb.init(project=\"data_ft_phi3LoRA\", job_type=\"split_data\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"data_splitted\", \n",
    "        type=\"dataset\",\n",
    "        description=\"Using the data we finetune\",\n",
    "    )\n",
    "    at.add_file(\"data_train.jsonl\")\n",
    "    at.add_file(\"data_eval.jsonl\")\n",
    "    wandb.log_artifact(at)\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb58a76e-c50c-4431-8584-3a4597c2a0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''import json\n",
    "from wandb import Api\n",
    "\n",
    "api = Api()\n",
    "artifact = api.artifact('data_ft/data_splitted', type='dataset')\n",
    "dataset_dir = artifact.download()'''\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "    \n",
    "train_dataset = load_jsonl(\"data_train.jsonl\")\n",
    "eval_dataset = load_jsonl(\"data_eval.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78b4982-b683-41ee-bcec-8cfd22c2b4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.', 'input': \" Do you ever wish you had the power of artificial intelligence right at your fingertips? That's exactly what AI/ML API offers, with over a hundred AI models available for you to use.   Imagine, a single API key being the key to a treasure chest of AI tools. With AI/ML API you can easily integrate these models into your business.   Not only does AI/ML API allow you to harness the power of AI, but it helps you to cut down on software costs and boost your efficiency.   If you're an educator or an innovator, AI/ML API is perfect for you. It's time to change the digital landscape and enrich your classrooms or brainstorming sessions with the help of AI.   So why wait? Join the AI revolution today with AI/ML API and explore the countless possibilities that AI and ML can offer to your business or classroom.  \", 'output': \"['AI ML models', 'API key unlocking treasure chest', 'business owner reducing software costs', 'educators and innovators using AI', 'Joining the AI revolution']\"}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_prompts = [prompt_input(row) for row in train_dataset]\n",
    "eval_prompts = [prompt_input(row) for row in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.\n",
      "\n",
      "### Input:\n",
      " Do you ever wish you had the power of artificial intelligence right at your fingertips? That's exactly what AI/ML API offers, with over a hundred AI models available for you to use.   Imagine, a single API key being the key to a treasure chest of AI tools. With AI/ML API you can easily integrate these models into your business.   Not only does AI/ML API allow you to harness the power of AI, but it helps you to cut down on software costs and boost your efficiency.   If you're an educator or an innovator, AI/ML API is perfect for you. It's time to change the digital landscape and enrich your classrooms or brainstorming sessions with the help of AI.   So why wait? Join the AI revolution today with AI/ML API and explore the countless possibilities that AI and ML can offer to your business or classroom.  \n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_eos(ds):\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['AI ML models', 'API key unlocking treasure chest', 'business owner reducing software costs', 'educators and innovators using AI', 'Joining the AI revolution']</s>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)\n",
    "train_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
    "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.\n",
      "\n",
      "### Input:\n",
      " Do you ever wish you had the power of artificial intelligence right at your fingertips? That's exactly what AI/ML API offers, with over a hundred AI models available for you to use.   Imagine, a single API key being the key to a treasure chest of AI tools. With AI/ML API you can easily integrate these models into your business.   Not only does AI/ML API allow you to harness the power of AI, but it helps you to cut down on software costs and boost your efficiency.   If you're an educator or an innovator, AI/ML API is perfect for you. It's time to change the digital landscape and enrich your classrooms or brainstorming sessions with the help of AI.   So why wait? Join the AI revolution today with AI/ML API and explore the countless possibilities that AI and ML can offer to your business or classroom.  \n",
      "\n",
      "### Response:\n",
      "['AI ML models', 'API key unlocking treasure chest', 'business owner reducing software costs', 'educators and innovators using AI', 'Joining the AI revolution']</s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58586813-7918-4578-9583-df14c5a6a6ad",
   "metadata": {},
   "source": [
    "### Packing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316d169-4292-41aa-a42d-cd49620a881c",
   "metadata": {},
   "source": [
    "We will pack multiple short examples into a longer chunk, so we can train more efficiently!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6df54e-f6bc-4e56-aec2-014a19fc654c",
   "metadata": {},
   "source": [
    "The main idea here is that the instruction/output samples are short, so let's concatenate a bunch of them together separated by the `EOS` token. We can also pre-tokenize and pre-pack the dataset and make everything faster!  If we define a `max_seq_len = 1024` the code to pack would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a537da60-db69-42a7-8c66-0ea3756c2847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_sequence_len = 1024\n",
    "\n",
    "def pack(dataset, max_seq_len=max_sequence_len):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
    "    \n",
    "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
    "    return packed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c78b4e9-2c7f-4aa1-b738-be04bc55b06b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4002502\n",
      "Total number of tokens: 470285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3904"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "len(train_ds_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43713b3-9f65-42a6-8338-ab0601e5f476",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "As we are training for completion, the labels (or targets) will be the inputs shifted by one. We are going to train with regular Cross Entropy and predict the next token on this packed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "batch_size = 4  # I have an A100 GPU with 40GB of RAM 😎\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator 😎\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ccaf4-dfa1-4daa-9a6f-244c8cda7818",
   "metadata": {},
   "source": [
    "It is always a good idea to check how does a batch looks like, you can quickly do this by sampling from the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffced7ec-bd1b-42b0-be64-04f3fae5df84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 13866,   338,  ..., 29881, 30130,  1758],\n",
       "         [30130,  1056,  5951,  ...,   393,  8128,  4340],\n",
       "         [29889, 14350,   263,  ...,   525, 29923,   386],\n",
       "         [ 2050,   800,   310,  ...,   348,  8247, 30130]]),\n",
       " 'labels': tensor([[13866,   338,   385,  ..., 30130,  1758,   294],\n",
       "         [ 1056,  5951,   262,  ...,  8128,  4340,  3030],\n",
       "         [14350,   263,  2933,  ..., 29923,   386,   936],\n",
       "         [  800,   310, 15680,  ...,  8247, 30130,   330]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d15d30-58d2-465d-a9f4-be0eef2ea06b",
   "metadata": {},
   "source": [
    "We can alos decode the batch just to be super sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate search keywords for searching images/videos from the given pa'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"input_ids\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate search keywords for searching images/videos from the given paragr'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"labels\"][0])[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757e458-14fd-4dd3-861f-efad04dce787",
   "metadata": {},
   "source": [
    "I like storing all my hyperparameters into a `SimpleNamespace`, it's like a dict but with .dot attribute access. Then I can access my batch size by doing config.batch_size instead of config[\"batch_size\"]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id='microsoft/Phi-3-mini-128k-instruct',\n",
    "    dataset_name=\"data\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_sequence_len, # Lenght of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a7166a6-8e88-4f0d-bc0e-70aac292c645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train for 1464 steps and evaluate every epoch\n"
     ]
    }
   ],
   "source": [
    "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd0229-5e4c-4bb5-827b-309bdbb351df",
   "metadata": {},
   "source": [
    "We first get a pretrained model with some configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b181225",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.bb5bf1e4001277a606e11debca0ef80323e5f824.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.bb5bf1e4001277a606e11debca0ef80323e5f824.modeling_phi3:Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a677a69bac6b40a4899d0469c435749a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_id = 'microsoft/Phi-3-mini-128k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0,trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 3856.73M, Trainable: 35.65M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92663c-95a4-4ecc-a9af-7a68d9648271",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "We setup the standard optimization stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5efdd402-c851-47a5-9134-5b47b7d118e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24440392-9837-4cbb-873a-372f9f5aca20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing during training\n",
    "\n",
    "We are almost there, let's create a simple function to sample from the model now and then, to visualy see what the models is outputting!\n",
    "Let's wrap the model.generate method for simplicity. You can grab the defaults sampling parameters from the GenerationConfig and passing the corresponding model_id. This will grab you the defaults for parameters like temperature, top p, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ec41718-52c6-4335-a534-2790d03ba069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44601edc-db5b-40ea-bb74-9591ea4e7e50",
   "metadata": {},
   "source": [
    "LoL 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.bb5bf1e4001277a606e11debca0ef80323e5f824.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate search keywords for searching images/videos from the given paragraph. Consider relevant elements and keywords should capture the essence and context of the paragraph while aligning with the overall meaning of the input. Your output should be a list of python strings, and each string will correspond to a sentence in the input with a search keyword. All should have string values. The length of the list and the number of sentences in the given paragraph should be the same.\n",
      "\n",
      "### Input:\n",
      " 'Khóa học Kế toán 4 trong 1' là một chương trình toàn diện bao gồm tất cả các khía cạnh kế toán trong bốn phần chính.   Phần đầu tiên của khóa học tập trung vào việc nắm vững tất cả các tài liệu kế toán như hóa đơn, sao kê ngân hàng, bảng lương, bảo hiểm, xuất nhập khẩu trong vòng 3 giờ.   Phần thứ hai nhấn mạnh đến các công việc kế toán mà doanh nghiệp mới thành lập cần quan tâm, bao gồm khai thuế và lựa chọn ngân hàng phù hợp.   Phần thứ ba của khóa học tìm hiểu cách doanh nghiệp có thể vay các khoản vay không có bảo đảm từ 500 triệu đến 2 tỷ đồng.   Phần cuối cùng của khóa học thảo luận về các tác động về thuế đối với các chi nhánh và KOC, đồng thời cung cấp các giải pháp tối ưu để quản lý các loại thuế này.  \n",
      "\n",
      "### Response:\n",
      "[\"Khóa học Kế toán 4 trong 1\", \"tất cả các khía cạnh kế toán\", \"tất cả các tài liệu kế toán\", \"hóa đơn, sao kê ngân hàng\", \"bảng lương, bảo hiểm, xuất nhập khẩu\", \"vòng 3 giờ\", \"khẩu vào doanh\n"
     ]
    }
   ],
   "source": [
    "prompt = eval_dataset[14][\"prompt\"]\n",
    "print(prompt + generate(prompt, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567920c0-005d-419d-98ab-4d797b243300",
   "metadata": {},
   "source": [
    "We can log a Table with those results to the project every X steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({table_name:table})\n",
    "    return table\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53aeac2a-1eb6-48cc-bc97-de038a80530c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Initialize the ROUGE scorer for bigrams (ROUGE-2)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "\n",
    "def compute_rouge2(predictions, references):\n",
    "    rouge2_scores = []\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)['rouge2'].fmeasure\n",
    "        rouge2_scores.append(score)\n",
    "    return sum(rouge2_scores) / len(rouge2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fbc09-5d65-4265-abce-adda01d85584",
   "metadata": {},
   "source": [
    "You can also quickly add validation if you feel so, the table can be also created at this step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6891b2c0-f22e-4647-9ac2-e07a994f3e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    eval_acc = Accuracy()\n",
    "    loss, total_steps = 0., 0\n",
    "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
    "        pbar.set_description(f\"doing validation\")\n",
    "        batch = to_gpu(batch)\n",
    "        total_steps += 1\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        predictions = tokenizer.batch_decode(out.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "        references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "        # Compute ROUGE-2 score\n",
    "        rouge2_score = compute_rouge2(predictions, references)\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
    "               \"eval/rouge2_score\": rouge2_score})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93834c0b-15e1-4e43-8535-dbb9f36af29d",
   "metadata": {},
   "source": [
    "Let's define a loop that compute evaluation and logs a Table with model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90177c-0c5a-48e7-9a39-2b9e67794814",
   "metadata": {},
   "source": [
    "## The actual Loop\n",
    "It's actually nothing fancy, and very short! It has:\n",
    "- Gradient accumulation and gradient scaling\n",
    "- sampling and model checkpoint saving (this trains very fast, no need to save multiple checkpoints)\n",
    "- We compute token accuracy, better metric than loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe75078a-5d35-46bc-8f33-0e3adf52d072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/FineTuning/wandb/run-20240619_051857-seguuftf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/seguuftf' target=\"_blank\">sweet-microwave-27</a></strong> to <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/seguuftf' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/seguuftf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a333f9311a44809add0be0dc57c2b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4cd0a7a45a4ffc83f6a262d489dda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb98e14424714383ba109d7f40d3b0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff68f8758ca14dee8ebb6b7a4cc4869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/976 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"data_ft_phi3LoRA\", # the project I am working on\n",
    "           tags=[\"baseline\",\"8b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # Convert logits to predictions and references to text\n",
    "            predictions = tokenizer.batch_decode(out.logits.argmax(dim=-1), skip_special_tokens=True)\n",
    "            references = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "            \n",
    "            # Compute ROUGE-2 score\n",
    "            rouge2_score = compute_rouge2(predictions, references)\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/rouge2_score\": rouge2_score,\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "    validate()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4099bc28-e695-46a6-994c-b612f7811937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.372 MB of 0.372 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▃▁</td></tr><tr><td>eval/rouge2_score</td><td>█▁▅</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▆██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▄▄▃▁▂▄▄▃▄▂▃▄▅▂▂▂▁▄▄▂▃▄▂▁▂▄▄▅▃▂▂▃▁▁▂▁▁▂</td></tr><tr><td>train/rouge2_score</td><td>▁▂▄▄▆▆▅▃▅▃▅▇▂▅▃▅▆▆▆▅▆▆▆▅▅▆▆▄▆▅▆▅█▆▅▆▇▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.11701</td></tr><tr><td>eval/rouge2_score</td><td>0.52899</td></tr><tr><td>train/global_step</td><td>1463</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.26578</td></tr><tr><td>train/rouge2_score</td><td>0.49473</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-microwave-27</strong> at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/seguuftf' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/seguuftf</a><br/> View project at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a><br/>Synced 5 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_051857-seguuftf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we save the model checkpoint at the end\n",
    "#save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec95b95-3e81-4a40-8eea-34048c992ba9",
   "metadata": {},
   "source": [
    "This trains in around 60 minutes on an A100. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936e007-47fa-400f-873c-5e038bd685c5",
   "metadata": {},
   "source": [
    "## Full Eval Dataset evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fafb9-b41f-401b-a1c4-37e1ede2e639",
   "metadata": {},
   "source": [
    "Let's log a table with model predictions on the eval_dataset (or at least the 250 first samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b89717a-ac70-43e8-9b7c-edd8d2c54cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/FineTuning/wandb/run-20240619_061840-krv6pj5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/krv6pj5i' target=\"_blank\">fiery-blaze-28</a></strong> to <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/krv6pj5i' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/krv6pj5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69af280e087f48e9ab5ad46efea04cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/tmp/ipykernel_13384/721575607.py\", line 5, in <module>\n",
      "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")\n",
      "  File \"/var/tmp/ipykernel_13384/3776927856.py\", line 8, in prompt_table\n",
      "    out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
      "  File \"/var/tmp/ipykernel_13384/4108886549.py\", line 4, in generate\n",
      "    output = model.generate(tokenized_prompt,\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 647, in generate\n",
      "    return self.get_base_model().generate(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1758, in generate\n",
      "    result = self._sample(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2397, in _sample\n",
      "    outputs = self(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jupyter/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py\", line 1286, in forward\n",
      "    outputs = self.model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jupyter/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py\", line 1164, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jupyter/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py\", line 903, in forward\n",
      "    if output_attentions:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-blaze-28</strong> at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/krv6pj5i' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA/runs/krv6pj5i</a><br/> View project at: <a href='https://wandb.ai/ps-ml-team/data_ft_phi3LoRA' target=\"_blank\">https://wandb.ai/ps-ml-team/data_ft_phi3LoRA</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240619_061840-krv6pj5i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_ft_phi3LoRA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# the project I am working on\u001b[39;00m\n\u001b[1;32m      2\u001b[0m            job_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m            config\u001b[38;5;241m=\u001b[39mconfig): \u001b[38;5;66;03m# the Hyperparameters I want to keep track of\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval();\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mprompt_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_predictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mprompt_table\u001b[0;34m(examples, log, table_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m tqdm(examples, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      7\u001b[0m     prompt, gpt4_output \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     table\u001b[38;5;241m.\u001b[39madd_data(prompt, out, prompt\u001b[38;5;241m+\u001b[39mout, gpt4_output, test_config\u001b[38;5;241m.\u001b[39mmax_new_tokens, test_config\u001b[38;5;241m.\u001b[39mgen_config\u001b[38;5;241m.\u001b[39mtemperature, test_config\u001b[38;5;241m.\u001b[39mgen_config\u001b[38;5;241m.\u001b[39mtop_p)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log:\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, max_new_tokens, gen_config)\u001b[0m\n\u001b[1;32m      2\u001b[0m tokenized_prompt \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(tokenized_prompt[\u001b[38;5;241m0\u001b[39m]):], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:647\u001b[0m, in \u001b[0;36mPeftModel.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py:1286\u001b[0m, in \u001b[0;36mPhi3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1283\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1299\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py:1164\u001b[0m, in \u001b[0;36mPhi3Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1155\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1156\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         use_cache,\n\u001b[1;32m   1162\u001b[0m     )\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1173\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/bb5bf1e4001277a606e11debca0ef80323e5f824/modeling_phi3.py:903\u001b[0m, in \u001b[0;36mPhi3DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_mlp_dropout(hidden_states)\n\u001b[1;32m    901\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    904\u001b[0m     outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (self_attn_weights,)\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with wandb.init(project=\"data_ft_phi3LoRA\", # the project I am working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the Hyperparameters I want to keep track of\n",
    "    model.eval();\n",
    "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59c93a50-049f-4c69-a392-7608be93254a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned/data_new_ft_phi3LoRA/tokenizer_config.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA/special_tokens_map.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA/tokenizer.model',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA/added_tokens.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.save_pretrained(\"fine_tuned/data_new_ft_phi3LoRA\")\n",
    "tokenizer.save_pretrained('fine_tuned/data_new_ft_phi3LoRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98a3730f-115b-41a0-8635-ab5fa15dd92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned/data_new_ft_phi3LoRA_ada/tokenizer_config.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA_ada/special_tokens_map.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA_ada/tokenizer.model',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA_ada/added_tokens.json',\n",
       " 'fine_tuned/data_new_ft_phi3LoRA_ada/tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('fine_tuned/data_new_ft_phi3LoRA_ada')\n",
    "\n",
    "tokenizer.save_pretrained('fine_tuned/data_new_ft_phi3LoRA_ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e9834e2-7d80-4d44-8b22-bb2e32c0520a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ef55ad8e14be382a02c7cd9a6ae3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"fine_tuned/data_new_ft_phi3LoRA\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned/data_new_ft_phi3LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5af96c75-0d9d-445e-833c-3073cb1ac6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ec87c0942744afbd680f223489bc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac7fa9487ad401fb310acd3327003cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e111c51dbe564c149a433b07481e7a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9406deafdd4ac393758684d1f04721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a95517cfd54ca6ab667e0c26f8584d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/143M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dbcbfcca4d49ea82a5c7cb08f07a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d62dc11f90a46bbb7125c83aafe20bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40016ca85e06472498bc2a5d4a05e093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/FineTuning/phi3'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "#Download the merged model\n",
    "model_id=\"Anishproshort/data_new_ft_phi3LoRA\"\n",
    "#Download the repository to local_dir\n",
    "snapshot_download(repo_id=model_id, local_dir=\"phi3\",\n",
    "                  local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "669b4162-5192-4131-879b-0fe2d70066a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: phi3-ft-LoRA-full-model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 1\n",
      "INFO:gguf.vocab:Setting special token type eos to 32000\n",
      "INFO:gguf.vocab:Setting special token type unk to 0\n",
      "INFO:gguf.vocab:Setting special token type pad to 32000\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "INFO:hf-to-gguf:Exporting model to 'phi3-ft.gguf'\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {3072, 32064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_qkv.weight,     torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.28.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.29.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.30.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.float32 --> F16, shape = {8192, 3072}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.float32 --> F16, shape = {3072, 16384}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.float32 --> F16, shape = {3072, 3072}\n",
      "INFO:hf-to-gguf:blk.31.attn_qkv.weight,    torch.float32 --> F16, shape = {3072, 9216}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.float32 --> F16, shape = {3072, 32064}\n",
      "Writing: 100%|███████████████████████████| 7.64G/7.64G [00:23<00:00, 323Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to 'phi3-ft.gguf'\n"
     ]
    }
   ],
   "source": [
    "!python llama.cpp/convert-hf-to-gguf.py phi3-ft-LoRA-full-model --outfile \"phi3-ft.gguf\" --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "055fca99-638a-477a-8f15-978aa11dc46c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./llama.cpp/examples/quantize/quantize.cpp: line 12: struct: command not found\n",
      "./llama.cpp/examples/quantize/quantize.cpp: line 13: std::string: command not found\n",
      "./llama.cpp/examples/quantize/quantize.cpp: line 14: llama_ftype: command not found\n",
      "./llama.cpp/examples/quantize/quantize.cpp: line 15: std::string: command not found\n",
      "./llama.cpp/examples/quantize/quantize.cpp: line 16: syntax error near unexpected token `}'\n",
      "./llama.cpp/examples/quantize/quantize.cpp: line 16: `};'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!./llama.cpp/examples/quantize/quantize.cpp \"phi3-ft.bin\" \"phi3-ft-Q5_K_M.gguf\" \"q5_k_m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "561a9fde-9bc1-4c12-807f-a1e40aeae77a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!sudo chmod +x llama.cpp/llama.cpp"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
